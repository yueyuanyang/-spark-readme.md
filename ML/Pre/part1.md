## spark 分词

### Spark下四种中文分词工具使用

- hanLP
- ansj
- jieba
- fudannlp

本文使用maven 导入四种分词工具

```
 <dependency>
        <groupId>org.ansj</groupId>
        <artifactId>ansj_seg</artifactId>
        <version>5.1.3</version>
    </dependency>
    <dependency>
        <groupId>com.hankcs</groupId>
        <artifactId>hanlp</artifactId>
        <version>portable-1.3.4</version>
    </dependency>
    <dependency>
        <groupId>com.huaban</groupId>
        <artifactId>jieba-analysis</artifactId>
        <version>1.0.2</version>
    </dependency>
```
fudannlp github地址：https://github.com/FudanNLP/fnlp 

模型文件国内网盘地址：https://pan.baidu.com/disk/home#list/vmode=list&path=%2F%E6%88%91%E7%9A%84%E8%B5%84%E6%BA%90%2Ffnlp%E7%BD%91%E7%9B%98%E9%95%9C%E5%83%8F

###

